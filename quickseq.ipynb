{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quickseq\n",
    "\n",
    "Determine tissue samples from an RNA-Seq dataset using alignment free methods.\n",
    "\n",
    "quickseq is being prepared for the 2018 BME Bootcamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "quickseq is composed of a few important parts, which are used to compare the samples in the provided dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcript Quantification\n",
    "\n",
    "For quantifying transcripts in the sample, we can use a variety of methods, and if quickseq is deemed valuable we can make the quantification methods configurable.\n",
    "\n",
    "From https://github.com/COMBINE-lab/salmon/releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-09-18 20:22:06--  https://github.com/COMBINE-lab/salmon/releases/download/v0.11.3/salmon-0.11.3-linux_x86_64.tar.gz\n",
      "Resolving github.com (github.com)... 192.30.255.112, 192.30.255.113\n",
      "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/32549942/22343d00-abd6-11e8-8376-9b332c400ee0?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20180919%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20180919T032206Z&X-Amz-Expires=300&X-Amz-Signature=0c568d71ebe4011fd14f9151d6cbd72124841229908e4bdefb74d4317b512f76&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dsalmon-0.11.3-linux_x86_64.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2018-09-18 20:22:06--  https://github-production-release-asset-2e65be.s3.amazonaws.com/32549942/22343d00-abd6-11e8-8376-9b332c400ee0?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20180919%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20180919T032206Z&X-Amz-Expires=300&X-Amz-Signature=0c568d71ebe4011fd14f9151d6cbd72124841229908e4bdefb74d4317b512f76&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dsalmon-0.11.3-linux_x86_64.tar.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.229.243\n",
      "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.229.243|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 34421241 (33M) [application/octet-stream]\n",
      "Saving to: ‘salmon-0.11.3-linux_x86_64.tar.gz’\n",
      "\n",
      "salmon-0.11.3-linux 100%[===================>]  32.83M  1.06MB/s    in 44s     \n",
      "\n",
      "2018-09-18 20:22:51 (759 KB/s) - ‘salmon-0.11.3-linux_x86_64.tar.gz’ saved [34421241/34421241]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/COMBINE-lab/salmon/releases/download/v0.11.3/salmon-0.11.3-linux_x86_64.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salmon-0.11.3-linux_x86_64/\n",
      "salmon-0.11.3-linux_x86_64/lib/\n",
      "salmon-0.11.3-linux_x86_64/lib/libtbbmalloc.so\n",
      "salmon-0.11.3-linux_x86_64/lib/libtbb.so\n",
      "salmon-0.11.3-linux_x86_64/lib/libgomp.so.1\n",
      "salmon-0.11.3-linux_x86_64/lib/liblzma.so.0\n",
      "salmon-0.11.3-linux_x86_64/lib/libtbbmalloc_proxy.so.2\n",
      "salmon-0.11.3-linux_x86_64/lib/libtbbmalloc_proxy.so\n",
      "salmon-0.11.3-linux_x86_64/lib/libtbb.so.2\n",
      "salmon-0.11.3-linux_x86_64/lib/libgcc_s.so.1\n",
      "salmon-0.11.3-linux_x86_64/lib/libm.so.6\n",
      "salmon-0.11.3-linux_x86_64/lib/libtbbmalloc.so.2\n",
      "salmon-0.11.3-linux_x86_64/bin/\n",
      "salmon-0.11.3-linux_x86_64/bin/salmon\n",
      "salmon-0.11.3-linux_x86_64/sample_data.tgz\n"
     ]
    }
   ],
   "source": [
    "!tar -xvf salmon-0.11.3-linux_x86_64.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing it out\n",
    "\n",
    "Let's see how salmon works really quick..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salmon v0.11.3\r\n",
      "\r\n",
      "Usage:  salmon -h|--help or \r\n",
      "        salmon -v|--version or \r\n",
      "        salmon -c|--cite or \r\n",
      "        salmon [--no-version-check] <COMMAND> [-h | options]\r\n",
      "\r\n",
      "Commands:\r\n",
      "     index Create a salmon index\r\n",
      "     quant Quantify a sample\r\n",
      "     alevin single cell analysis\r\n",
      "     swim  Perform super-secret operation\r\n",
      "     quantmerge Merge multiple quantifications into a single file\r\n"
     ]
    }
   ],
   "source": [
    "!./salmon-0.11.3-linux_x86_64/bin/salmon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to make an index, but we're really curious about quant, but can't we just try to swim first?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version Info: Could not resolve upgrade information in the alotted time.\r\n",
      "Check for upgrades manually at https://combine-lab.github.io/salmon\r\n",
      "\r\n",
      "    _____       __\r\n",
      "   / ___/____ _/ /___ ___  ____  ____\r\n",
      "   \\__ \\/ __ `/ / __ `__ \\/ __ \\/ __ \\\r\n",
      "  ___/ / /_/ / / / / / / / /_/ / / / /\r\n",
      " /____/\\__,_/_/_/ /_/ /_/\\____/_/ /_/\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!./salmon-0.11.3-linux_x86_64/bin/salmon swim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was hoping for fish. Let's get some test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!wget https://s3.amazonaws.com/nanopore-human-wgs/rna/fastq/UCSC_Run1_20170919_1D.pass.dedup.fastq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version Info: Could not resolve upgrade information in the alotted time.\r\n",
      "Check for upgrades manually at https://combine-lab.github.io/salmon\r\n",
      "    salmon v0.11.3\r\n",
      "    ===============\r\n",
      "\r\n",
      "    salmon quant has two modes --- one quantifies expression using raw reads\r\n",
      "    and the other makes use of already-aligned reads (in BAM/SAM format).\r\n",
      "    Which algorithm is used depends on the arguments passed to salmon quant.\r\n",
      "    If you provide salmon with alignments '-a [ --alignments ]' then the\r\n",
      "    alignment-based algorithm will be used, otherwise the algorithm for\r\n",
      "    quantifying from raw reads will be used.\r\n",
      "\r\n",
      "    to view the help for salmon's quasi-mapping-based mode, use the command\r\n",
      "\r\n",
      "    salmon quant --help-reads\r\n",
      "\r\n",
      "    To view the help for salmon's alignment-based mode, use the command\r\n",
      "\r\n",
      "    salmon quant --help-alignment\r\n",
      "\r\n",
      "    \r\n"
     ]
    }
   ],
   "source": [
    "!./salmon-0.11.3-linux_x86_64/bin/salmon quant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, that's us, raw reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version Info: Could not resolve upgrade information in the alotted time.\r\n",
      "Check for upgrades manually at https://combine-lab.github.io/salmon\r\n",
      "\r\n",
      "Quant\r\n",
      "==========\r\n",
      "Perform dual-phase, mapping-based estimation of\r\n",
      "transcript abundance from RNA-seq reads\r\n",
      "\r\n",
      "salmon quant options:\r\n",
      "\r\n",
      "\r\n",
      "mapping input options:\r\n",
      "  -l [ --libType ] arg                  Format string describing the library \r\n",
      "                                        type\r\n",
      "  -i [ --index ] arg                    salmon index\r\n",
      "  -r [ --unmatedReads ] arg             List of files containing unmated reads \r\n",
      "                                        of (e.g. single-end reads)\r\n",
      "  -1 [ --mates1 ] arg                   File containing the #1 mates\r\n",
      "  -2 [ --mates2 ] arg                   File containing the #2 mates\r\n",
      "\r\n",
      "\r\n",
      "basic options:\r\n",
      "  -v [ --version ]                      print version string\r\n",
      "  -h [ --help ]                         produce help message\r\n",
      "  -o [ --output ] arg                   Output quantification directory.\r\n",
      "  --seqBias                             Perform sequence-specific bias \r\n",
      "                                        correction.\r\n",
      "  --gcBias                              [beta for single-end reads] Perform \r\n",
      "                                        fragment GC bias correction\r\n",
      "  -p [ --threads ] arg (=8)             The number of threads to use \r\n",
      "                                        concurrently.\r\n",
      "  --incompatPrior arg (=0)              This option sets the prior probability \r\n",
      "                                        that an alignment that disagrees with \r\n",
      "                                        the specified library type (--libType) \r\n",
      "                                        results from the true fragment origin. \r\n",
      "                                        Setting this to 0 specifies that \r\n",
      "                                        alignments that disagree with the \r\n",
      "                                        library type should be \"impossible\", \r\n",
      "                                        while setting it to 1 says that \r\n",
      "                                        alignments that disagree with the \r\n",
      "                                        library type are no less likely than \r\n",
      "                                        those that do\r\n",
      "  -g [ --geneMap ] arg                  File containing a mapping of \r\n",
      "                                        transcripts to genes.  If this file is \r\n",
      "                                        provided salmon will output both \r\n",
      "                                        quant.sf and quant.genes.sf files, \r\n",
      "                                        where the latter contains aggregated \r\n",
      "                                        gene-level abundance estimates.  The \r\n",
      "                                        transcript to gene mapping should be \r\n",
      "                                        provided as either a GTF file, or a in \r\n",
      "                                        a simple tab-delimited format where \r\n",
      "                                        each line contains the name of a \r\n",
      "                                        transcript and the gene to which it \r\n",
      "                                        belongs separated by a tab.  The \r\n",
      "                                        extension of the file is used to \r\n",
      "                                        determine how the file should be \r\n",
      "                                        parsed.  Files ending in '.gtf', '.gff'\r\n",
      "                                        or '.gff3' are assumed to be in GTF \r\n",
      "                                        format; files with any other extension \r\n",
      "                                        are assumed to be in the simple format.\r\n",
      "                                        In GTF / GFF format, the \r\n",
      "                                        \"transcript_id\" is assumed to contain \r\n",
      "                                        the transcript identifier and the \r\n",
      "                                        \"gene_id\" is assumed to contain the \r\n",
      "                                        corresponding gene identifier.\r\n",
      "  --meta                                If you're using Salmon on a metagenomic\r\n",
      "                                        dataset, consider setting this flag to \r\n",
      "                                        disable parts of the abundance \r\n",
      "                                        estimation model that make less sense \r\n",
      "                                        for metagenomic data.\r\n",
      "\r\n",
      "\r\n",
      "options specific to mapping mode:\r\n",
      "  --discardOrphansQuasi                 [Quasi-mapping mode only] : Discard \r\n",
      "                                        orphan mappings in quasi-mapping mode. \r\n",
      "                                        If this flag is passed then only paired\r\n",
      "                                        mappings will be considered toward \r\n",
      "                                        quantification estimates.  The default \r\n",
      "                                        behavior is to consider orphan mappings\r\n",
      "                                        if no valid paired mappings exist.  \r\n",
      "                                        This flag is independent of the option \r\n",
      "                                        to write the orphaned mappings to file \r\n",
      "                                        (--writeOrphanLinks).\r\n",
      "  --validateMappings                    [Quasi-mapping mode only] : Validate \r\n",
      "                                        mappings using alignment-based \r\n",
      "                                        verifcation. If this flag is passed, \r\n",
      "                                        quasi-mappings will be validated to \r\n",
      "                                        ensure that they could give rise to a \r\n",
      "                                        reasonable alignment before they are \r\n",
      "                                        further used for quantification.\r\n",
      "  --consensusSlack arg (=0)             [Quasi-mapping mode only] : The amount \r\n",
      "                                        of slack allowed in the quasi-mapping \r\n",
      "                                        consensus mechanism.  Normally, a \r\n",
      "                                        transcript must cover all hits to be \r\n",
      "                                        considered for mapping.  If this is set\r\n",
      "                                        to a value, X, greater than 0, then a \r\n",
      "                                        transcript can fail to cover up to X \r\n",
      "                                        hits before it is discounted as a \r\n",
      "                                        mapping candidate.  The default value \r\n",
      "                                        of this option is 1 if \r\n",
      "                                        --validateMappings is given and 0 \r\n",
      "                                        otherwise.\r\n",
      "  --minScoreFraction arg (=0.65000000000000002)\r\n",
      "                                        [Quasi-mapping mode only] : The \r\n",
      "                                        fraction of the optimal possible \r\n",
      "                                        alignment score that a mapping must \r\n",
      "                                        achieve in order to be considered \r\n",
      "                                        \"valid\" --- should be in (0,1].\r\n",
      "  --ma arg (=\u0002)                         [Quasi-mapping mode only] : The value \r\n",
      "                                        given to a match between read and \r\n",
      "                                        reference nucleotides in an alignment.\r\n",
      "  --mp arg (=�)                         [Quasi-mapping mode only] : The value \r\n",
      "                                        given to a mis-match between read and \r\n",
      "                                        reference nucleotides in an alignment.\r\n",
      "  --go arg (=\u0005)                         [Quasi-mapping mode only] : The value \r\n",
      "                                        given to a gap opening in an alignment.\r\n",
      "  --ge arg (=\u0003)                         [Quasi-mapping mode only] : The value \r\n",
      "                                        given to a gap extension in an \r\n",
      "                                        alignment.\r\n",
      "  --allowOrphansFMD                     [FMD-mapping mode only] : Consider \r\n",
      "                                        orphaned reads as valid hits when \r\n",
      "                                        performing lightweight-alignment.  This\r\n",
      "                                        option will increase sensitivity (allow\r\n",
      "                                        more reads to map and more transcripts \r\n",
      "                                        to be detected), but may decrease \r\n",
      "                                        specificity as orphaned alignments are \r\n",
      "                                        more likely to be spurious.\r\n",
      "  -z [ --writeMappings ] [=arg(=-)]     If this option is provided, then the \r\n",
      "                                        quasi-mapping results will be written \r\n",
      "                                        out in SAM-compatible format.  By \r\n",
      "                                        default, output will be directed to \r\n",
      "                                        stdout, but an alternative file name \r\n",
      "                                        can be provided instead.\r\n",
      "  -c [ --consistentHits ]               Force hits gathered during \r\n",
      "                                        quasi-mapping to be \"consistent\" (i.e. \r\n",
      "                                        co-linear and approximately the right \r\n",
      "                                        distance apart).\r\n",
      "  --strictIntersect                     Modifies how orphans are assigned.  \r\n",
      "                                        When this flag is set, if the \r\n",
      "                                        intersection of the quasi-mappings for \r\n",
      "                                        the left and right is empty, then all \r\n",
      "                                        mappings for the left and all mappings \r\n",
      "                                        for the right read are reported as \r\n",
      "                                        orphaned quasi-mappings\r\n",
      "  --fasterMapping                       [Developer]: Disables some extra checks\r\n",
      "                                        during quasi-mapping. This may make \r\n",
      "                                        mapping a little bit faster at the \r\n",
      "                                        potential cost of returning too many \r\n",
      "                                        mappings (i.e. some sub-optimal \r\n",
      "                                        mappings) for certain reads.\r\n",
      "  -x [ --quasiCoverage ] arg (=0)       [Experimental]: The fraction of the \r\n",
      "                                        read that must be covered by MMPs (of \r\n",
      "                                        length >= 31) if this read is to be \r\n",
      "                                        considered as \"mapped\".  This may help \r\n",
      "                                        to avoid \"spurious\" mappings. A value \r\n",
      "                                        of 0 (the default) denotes no coverage \r\n",
      "                                        threshold (a single 31-mer can yield a \r\n",
      "                                        mapping).  Since coverage by exact \r\n",
      "                                        matching, large, MMPs is a rather \r\n",
      "                                        strict condition, this value should \r\n",
      "                                        likely be set to something low, if \r\n",
      "                                        used.\r\n",
      "\r\n",
      "\r\n",
      "advanced options:\r\n",
      "  --alternativeInitMode                 [Experimental]: Use an alternative \r\n",
      "                                        strategy (rather than simple \r\n",
      "                                        interpolation between) the online and \r\n",
      "                                        uniform abundance estimates to \r\n",
      "                                        initalize the EM / VBEM algorithm.\r\n",
      "  --auxDir arg (=aux_info)              The sub-directory of the quantification\r\n",
      "                                        directory where auxiliary information \r\n",
      "                                        e.g. bootstraps, bias parameters, etc. \r\n",
      "                                        will be written.\r\n",
      "  --dumpEq                              Dump the equivalence class counts that \r\n",
      "                                        were computed during quasi-mapping\r\n",
      "  -d [ --dumpEqWeights ]                Includes \"rich\" equivlance class \r\n",
      "                                        weights in the output when equivalence \r\n",
      "                                        class information is being dumped to \r\n",
      "                                        file.\r\n",
      "  --minAssignedFrags arg (=10)          The minimum number of fragments that \r\n",
      "                                        must be assigned to the transcriptome \r\n",
      "                                        for quantification to proceed.\r\n",
      "  --reduceGCMemory                      If this option is selected, a more \r\n",
      "                                        memory efficient (but slightly slower) \r\n",
      "                                        representation is used to compute \r\n",
      "                                        fragment GC content. Enabling this will\r\n",
      "                                        reduce memory usage, but can also \r\n",
      "                                        reduce speed.  However, the results \r\n",
      "                                        themselves will remain the same.\r\n",
      "  --biasSpeedSamp arg (=5)              The value at which the fragment length \r\n",
      "                                        PMF is down-sampled when evaluating \r\n",
      "                                        sequence-specific & GC fragment bias.  \r\n",
      "                                        Larger values speed up effective length\r\n",
      "                                        correction, but may decrease the \r\n",
      "                                        fidelity of bias modeling results.\r\n",
      "  --fldMax arg (=1000)                  The maximum fragment length to consider\r\n",
      "                                        when building the empirical \r\n",
      "                                        distribution\r\n",
      "  --fldMean arg (=250)                  The mean used in the fragment length \r\n",
      "                                        distribution prior\r\n",
      "  --fldSD arg (=25)                     The standard deviation used in the \r\n",
      "                                        fragment length distribution prior\r\n",
      "  -f [ --forgettingFactor ] arg (=0.65000000000000002)\r\n",
      "                                        The forgetting factor used in the \r\n",
      "                                        online learning schedule.  A smaller \r\n",
      "                                        value results in quicker learning, but \r\n",
      "                                        higher variance and may be unstable.  A\r\n",
      "                                        larger value results in slower learning\r\n",
      "                                        but may be more stable.  Value should \r\n",
      "                                        be in the interval (0.5, 1.0].\r\n",
      "  --initUniform                         initialize the offline inference with \r\n",
      "                                        uniform parameters, rather than seeding\r\n",
      "                                        with online parameters.\r\n",
      "  -w [ --maxReadOcc ] arg (=200)        Reads \"mapping\" to more than this many \r\n",
      "                                        places won't be considered.\r\n",
      "  --noLengthCorrection                  [experimental] : Entirely disables \r\n",
      "                                        length correction when estimating the \r\n",
      "                                        abundance of transcripts.  This option \r\n",
      "                                        can be used with protocols where one \r\n",
      "                                        expects that fragments derive from \r\n",
      "                                        their underlying targets without regard\r\n",
      "                                        to that target's length (e.g. QuantSeq)\r\n",
      "  --noEffectiveLengthCorrection         Disables effective length correction \r\n",
      "                                        when computing the probability that a \r\n",
      "                                        fragment was generated from a \r\n",
      "                                        transcript.  If this flag is passed in,\r\n",
      "                                        the fragment length distribution is not\r\n",
      "                                        taken into account when computing this \r\n",
      "                                        probability.\r\n",
      "  --noFragLengthDist                    [experimental] : Don't consider \r\n",
      "                                        concordance with the learned fragment \r\n",
      "                                        length distribution when trying to \r\n",
      "                                        determine the probability that a \r\n",
      "                                        fragment has originated from a \r\n",
      "                                        specified location.  Normally, \r\n",
      "                                        Fragments with unlikely lengths will be\r\n",
      "                                        assigned a smaller relative probability\r\n",
      "                                        than those with more likely lengths.  \r\n",
      "                                        When this flag is passed in, the \r\n",
      "                                        observed fragment length has no effect \r\n",
      "                                        on that fragment's a priori \r\n",
      "                                        probability.\r\n",
      "  --noBiasLengthThreshold               [experimental] : If this option is \r\n",
      "                                        enabled, then no (lower) threshold will\r\n",
      "                                        be set on how short bias correction can\r\n",
      "                                        make effective lengths. This can \r\n",
      "                                        increase the precision of bias \r\n",
      "                                        correction, but harm robustness.  The \r\n",
      "                                        default correction applies a threshold.\r\n",
      "  --numBiasSamples arg (=2000000)       Number of fragment mappings to use when\r\n",
      "                                        learning the sequence-specific bias \r\n",
      "                                        model.\r\n",
      "  --numAuxModelSamples arg (=5000000)   The first <numAuxModelSamples> are used\r\n",
      "                                        to train the auxiliary model parameters\r\n",
      "                                        (e.g. fragment length distribution, \r\n",
      "                                        bias, etc.).  After ther first \r\n",
      "                                        <numAuxModelSamples> observations the \r\n",
      "                                        auxiliary model parameters will be \r\n",
      "                                        assumed to have converged and will be \r\n",
      "                                        fixed.\r\n",
      "  --numPreAuxModelSamples arg (=1000000)\r\n",
      "                                        The first <numPreAuxModelSamples> will \r\n",
      "                                        have their assignment likelihoods and \r\n",
      "                                        contributions to the transcript \r\n",
      "                                        abundances computed without applying \r\n",
      "                                        any auxiliary models.  The purpose of \r\n",
      "                                        ignoring the auxiliary models for the \r\n",
      "                                        first <numPreAuxModelSamples> \r\n",
      "                                        observations is to avoid applying these\r\n",
      "                                        models before thier parameters have \r\n",
      "                                        been learned sufficiently well.\r\n",
      "  --useEM                               Use the traditional EM algorithm for \r\n",
      "                                        optimization in the batch passes.\r\n",
      "  --useVBOpt                            Use the Variational Bayesian EM \r\n",
      "                                        [default]\r\n",
      "  --rangeFactorizationBins arg (=0)     Factorizes the likelihood used in \r\n",
      "                                        quantification by adopting a new notion\r\n",
      "                                        of equivalence classes based on the \r\n",
      "                                        conditional probabilities with which \r\n",
      "                                        fragments are generated from different \r\n",
      "                                        transcripts.  This is a more \r\n",
      "                                        fine-grained factorization than the \r\n",
      "                                        normal rich equivalence classes.  The \r\n",
      "                                        default value (0) corresponds to the \r\n",
      "                                        standard rich equivalence classes, and \r\n",
      "                                        larger values imply a more fine-grained\r\n",
      "                                        factorization.  If range factorization \r\n",
      "                                        is enabled, a common value to select \r\n",
      "                                        for this parameter is 4.\r\n",
      "  --numGibbsSamples arg (=0)            Number of Gibbs sampling rounds to \r\n",
      "                                        perform.\r\n",
      "  --noGammaDraw                         This switch will disable drawing \r\n",
      "                                        transcript fractions from a Gamma \r\n",
      "                                        distribution during Gibbs sampling.  In\r\n",
      "                                        this case the sampler does not account \r\n",
      "                                        for shot-noise, but only assignment \r\n",
      "                                        ambiguity\r\n",
      "  --numBootstraps arg (=0)              Number of bootstrap samples to \r\n",
      "                                        generate. Note: This is mutually \r\n",
      "                                        exclusive with Gibbs sampling.\r\n",
      "  --bootstrapReproject                  This switch will learn the parameter \r\n",
      "                                        distribution from the bootstrapped \r\n",
      "                                        counts for each sample, but will \r\n",
      "                                        reproject those parameters onto the \r\n",
      "                                        original equivalence class counts.\r\n",
      "  --thinningFactor arg (=16)            Number of steps to discard for every \r\n",
      "                                        sample kept from the Gibbs chain. The \r\n",
      "                                        larger this number, the less chance \r\n",
      "                                        that subsequent samples are \r\n",
      "                                        auto-correlated, but the slower \r\n",
      "                                        sampling becomes.\r\n",
      "  -q [ --quiet ]                        Be quiet while doing quantification \r\n",
      "                                        (don't write informative output to the \r\n",
      "                                        console unless something goes wrong).\r\n",
      "  --perTranscriptPrior                  The prior (either the default or the \r\n",
      "                                        argument provided via --vbPrior) will \r\n",
      "                                        be interpreted as a transcript-level \r\n",
      "                                        prior (i.e. each transcript will be \r\n",
      "                                        given a prior read count of this value)\r\n",
      "  --sigDigits arg (=3)                  The number of significant digits to \r\n",
      "                                        write when outputting the \r\n",
      "                                        EffectiveLength and NumReads columns\r\n",
      "  --vbPrior arg (=1.0000000000000001e-05)\r\n",
      "                                        The prior that will be used in the VBEM\r\n",
      "                                        algorithm.  This is interpreted as a \r\n",
      "                                        per-nucleotide prior, unless the \r\n",
      "                                        --perTranscriptPrior flag is also \r\n",
      "                                        given, in which case this is used as a \r\n",
      "                                        transcript-level prior\r\n",
      "  --writeOrphanLinks                    Write the transcripts that are linked \r\n",
      "                                        by orphaned reads.\r\n",
      "  --writeUnmappedNames                  Write the names of un-mapped reads to \r\n",
      "                                        the file unmapped_names.txt in the \r\n",
      "                                        auxiliary directory.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!./salmon-0.11.3-linux_x86_64/bin/salmon quant --help-reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll grab a transcriptome from gencode https://www.gencodegenes.org/releases/current.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-09-18 20:30:40--  ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_28/gencode.v28.transcripts.fa.gz\n",
      "           => ‘gencode.v28.transcripts.fa.gz’\n",
      "Resolving ftp.ebi.ac.uk (ftp.ebi.ac.uk)... 193.62.192.4\n",
      "Connecting to ftp.ebi.ac.uk (ftp.ebi.ac.uk)|193.62.192.4|:21... connected.\n",
      "Logging in as anonymous ... Logged in!\n",
      "==> SYST ... done.    ==> PWD ... done.\n",
      "==> TYPE I ... done.  ==> CWD (1) /pub/databases/gencode/Gencode_human/release_28 ... done.\n",
      "==> SIZE gencode.v28.transcripts.fa.gz ... 64672964\n",
      "==> PASV ... done.    ==> RETR gencode.v28.transcripts.fa.gz ... done.\n",
      "Length: 64672964 (62M) (unauthoritative)\n",
      "\n",
      "gencode.v28.transcr 100%[===================>]  61.68M   425KB/s    in 2m 48s  \n",
      "\n",
      "2018-09-18 20:33:30 (375 KB/s) - ‘gencode.v28.transcripts.fa.gz’ saved [64672964]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_28/gencode.v28.transcripts.fa.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll make this index, that creates windowed k-mer view of the reference transcriptome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version Info: Could not resolve upgrade information in the alotted time.\n",
      "Check for upgrades manually at https://combine-lab.github.io/salmon\n",
      "index [\"gencode.v28.transcripts.fa.gz.index\"] did not previously exist  . . . creating it\n",
      "[2018-09-18 20:36:44.040] [jLog] [info] building index\n",
      "\u001b[00m[2018-09-18 20:36:44.040] [jointLog] [info] [Step 1 of 4] : counting k-mers\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:45.421] [jointLog] [warning] Entry with header [ENST00000473810.1|ENSG00000239255.1|OTTHUMG00000157482.1|OTTHUMT00000348942.1|RP11-145M9.2-001|RP11-145M9.2|25|processed_pseudogene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:45.512] [jointLog] [warning] Entry with header [ENST00000603775.1|ENSG00000271544.1|OTTHUMG00000184300.1|OTTHUMT00000468575.1|AC006499.9-001|AC006499.9|23|processed_pseudogene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:46.546] [jointLog] [warning] Entry with header [ENST00000632684.1|ENSG00000282431.1|OTTHUMG00000190602.2|OTTHUMT00000485301.2|RP11-520H11.10-001|TRBD1|12|TR_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:47.875] [jointLog] [warning] Entry with header [ENST00000626826.1|ENSG00000281344.1|OTTHUMG00000189570.1|OTTHUMT00000479989.1|RP11-210L7.2-001|HELLPAR|205012|macro_lncRNA|] was longer than 200000 nucleotides.  Are you certain that we are indexing a transcriptome and not a genome?\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:47.934] [jointLog] [warning] Entry with header [ENST00000543745.1|ENSG00000255972.1|OTTHUMG00000168883.1|OTTHUMT00000401485.1|RP11-324E6.8-001|RP11-324E6.8|28|processed_pseudogene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.088] [jointLog] [warning] Entry with header [ENST00000415118.1|ENSG00000223997.1|OTTHUMG00000170844.2|OTTHUMT00000410670.2|AE000661.52-001|TRDD1|8|TR_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.088] [jointLog] [warning] Entry with header [ENST00000434970.2|ENSG00000237235.2|OTTHUMG00000170845.2|OTTHUMT00000410671.2|AE000661.53-001|TRDD2|9|TR_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.088] [jointLog] [warning] Entry with header [ENST00000448914.1|ENSG00000228985.1|OTTHUMG00000170846.2|OTTHUMT00000410672.2|AE000661.54-001|TRDD3|13|TR_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000439842.1|ENSG00000236597.1|OTTHUMG00000152435.2|OTTHUMT00000326213.2|AL122127.38-001|IGHD7-27|11|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000390567.1|ENSG00000211907.1|OTTHUMG00000152429.2|OTTHUMT00000326207.2|AL122127.37-001|IGHD1-26|20|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000452198.1|ENSG00000225825.1|OTTHUMG00000152436.2|OTTHUMT00000326214.2|AL122127.36-001|IGHD6-25|18|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000390569.1|ENSG00000211909.1|OTTHUMG00000152427.2|OTTHUMT00000326205.2|AL122127.35-001|IGHD5-24|20|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000437320.1|ENSG00000227196.1|OTTHUMG00000152438.2|OTTHUMT00000326216.2|AL122127.34-001|IGHD4-23|19|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000390572.1|ENSG00000211912.1|OTTHUMG00000152428.2|OTTHUMT00000326206.2|AL122127.32-001|IGHD2-21|28|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000450276.1|ENSG00000237020.1|OTTHUMG00000152432.2|OTTHUMT00000326210.2|AL122127.31-001|IGHD1-20|17|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000390574.1|ENSG00000211914.1|OTTHUMG00000152431.2|OTTHUMT00000326209.2|AL122127.30-001|IGHD6-19|21|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000390575.1|ENSG00000211915.1|OTTHUMG00000152433.2|OTTHUMT00000326211.2|AL122127.29-001|IGHD5-18|20|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000431870.1|ENSG00000227800.1|OTTHUMG00000152437.2|OTTHUMT00000326215.2|AL122127.28-001|IGHD4-17|16|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000451044.1|ENSG00000227108.1|OTTHUMG00000152369.2|OTTHUMT00000326003.2|AB019441.47-001|IGHD1-14|17|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000390580.1|ENSG00000211920.1|OTTHUMG00000152370.2|OTTHUMT00000326004.2|AB019441.46-001|IGHD6-13|21|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000390581.1|ENSG00000211921.1|OTTHUMG00000152367.2|OTTHUMT00000326001.2|AB019441.45-001|IGHD5-12|23|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000431440.2|ENSG00000232543.2|OTTHUMG00000152368.2|OTTHUMT00000326002.2|AB019441.44-001|IGHD4-11|16|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000430425.1|ENSG00000237197.1|OTTHUMG00000152357.2|OTTHUMT00000325963.2|AB019441.40-001|IGHD1-7|17|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000454691.1|ENSG00000228131.1|OTTHUMG00000152353.2|OTTHUMT00000325959.2|AB019441.39-001|IGHD6-6|18|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000390588.1|ENSG00000211928.1|OTTHUMG00000152360.2|OTTHUMT00000325966.2|AB019441.38-001|IGHD5-5|20|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000414852.1|ENSG00000233655.1|OTTHUMG00000152355.2|OTTHUMT00000325961.2|AB019441.37-001|IGHD4-4|16|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000454908.1|ENSG00000236170.1|OTTHUMG00000152359.2|OTTHUMT00000325965.2|AB019441.34-001|IGHD1-1|17|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000518246.1|ENSG00000254045.1|OTTHUMG00000152060.1|OTTHUMT00000325154.1|AB019439.71-001|IGHVIII-22-2|28|IG_V_pseudogene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.358] [jointLog] [warning] Entry with header [ENST00000604642.1|ENSG00000270961.1|OTTHUMG00000184622.2|OTTHUMT00000468982.2|RP11-1360M22.8-001|IGHD5OR15-5A|23|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.358] [jointLog] [warning] Entry with header [ENST00000603326.1|ENSG00000271317.1|OTTHUMG00000184621.3|OTTHUMT00000468981.3|RP11-1360M22.7-001|IGHD4OR15-4A|19|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.358] [jointLog] [warning] Entry with header [ENST00000605284.1|ENSG00000271336.1|OTTHUMG00000184580.2|OTTHUMT00000468908.2|RP11-1360M22.3-001|IGHD1OR15-1A|17|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.359] [jointLog] [warning] Entry with header [ENST00000604446.1|ENSG00000270824.1|OTTHUMG00000184624.2|OTTHUMT00000468984.2|RP11-810K23.15-001|IGHD5OR15-5B|23|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.359] [jointLog] [warning] Entry with header [ENST00000603693.1|ENSG00000270451.1|OTTHUMG00000184611.3|OTTHUMT00000468945.3|RP11-810K23.14-001|IGHD4OR15-4B|19|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.359] [jointLog] [warning] Entry with header [ENST00000604838.1|ENSG00000270185.1|OTTHUMG00000184585.2|OTTHUMT00000468915.2|RP11-1360M22.4-001|IGHD1OR15-1B|17|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.965] [jointLog] [warning] Entry with header [ENST00000579054.1|ENSG00000266416.1|OTTHUMG00000179204.1|OTTHUMT00000445280.1|RP1-66C13.2-001|RP1-66C13.2|28|processed_pseudogene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:49.475] [jointLog] [warning] Entry with header [ENST00000634174.1|ENSG00000282732.1|OTTHUMG00000191398.1|OTTHUMT00000487783.1|RP11-157B13.10-001|RP11-157B13.10|28|unprocessed_pseudogene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00mElapsed time: 6.2232s\n",
      "\n",
      "\u001b[33m\u001b[1m[2018-09-18 20:36:50.263] [jointLog] [warning] Removed 808 transcripts that were sequence duplicates of indexed transcripts.\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:50.263] [jointLog] [warning] If you wish to retain duplicate transcripts, please use the `--keepDuplicates` flag\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:36:50.264] [jointLog] [info] Replaced 4 non-ATCG nucleotides\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:36:50.264] [jointLog] [info] Clipped poly-A tails from 1,586 transcripts\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:36:50.275] [jointLog] [info] Building rank-select dictionary and saving to disk\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:36:50.291] [jointLog] [info] done\n",
      "\u001b[00mElapsed time: 0.0159987s\n",
      "\u001b[00m[2018-09-18 20:36:50.291] [jointLog] [info] Writing sequence data to file . . . \n",
      "\u001b[00m\u001b[00m[2018-09-18 20:36:50.470] [jointLog] [info] done\n",
      "\u001b[00mElapsed time: 0.178981s\n",
      "\u001b[00m[2018-09-18 20:36:50.487] [jointLog] [info] Building 32-bit suffix array (length of generalized text is 308,972,089)\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:36:50.778] [jointLog] [info] Building suffix array . . . \n",
      "\u001b[00msuccess\n",
      "saving to disk . . . done\n",
      "Elapsed time: 0.664563s\n",
      "done\n",
      "Elapsed time: 29.9049s\n",
      "processed 130,000,000 positions"
     ]
    }
   ],
   "source": [
    "!./salmon-0.11.3-linux_x86_64/bin/salmon index -t gencode.v28.transcripts.fa.gz -i gencode.v28.transcripts.fa.gz.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So that's a thing that takes a long time but you only have to build once per transcriptome. We probably could have sped things up if we have picked 9/18/18 8:37 PM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88430842 0.67216518 0.92837791]\n",
      "[3.195872813441972, 2.455719077557213, 3.7154725627826517, 2.6691288682886327, 2.9793831559625517]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sklearn.metrics.pairwise\n",
    "import scipy.spatial.distance\n",
    "\n",
    "# this is a pretend transcript-sample matrix\n",
    "r = np.random.rand(5,3)\n",
    "\n",
    "print(r[0])\n",
    "\n",
    "# now calculate distance somehow...\n",
    "distances = sklearn.metrics.pairwise.euclidean_distances(r)\n",
    "\n",
    "print([sum(x) for x in distances])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = option1(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(r, 50, density=True, facecolor='g', alpha=0.75)\n",
    "\n",
    "\n",
    "plt.xlabel('Smarts')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Histogram of IQ')\n",
    "plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "plt.axis([40, 160, 0, 0.03])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
