{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quickseq\n",
    "\n",
    "Determine tissue samples from an RNA-Seq dataset using alignment free methods.\n",
    "\n",
    "quickseq is being prepared for the 2018 BME Bootcamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "quickseq is composed of a few important parts, which are used to compare the samples in the provided dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcript Quantification\n",
    "\n",
    "For quantifying transcripts in the sample, we can use a variety of methods, and if quickseq is deemed valuable we can make the quantification methods configurable.\n",
    "\n",
    "From https://github.com/COMBINE-lab/salmon/releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-09-18 20:22:06--  https://github.com/COMBINE-lab/salmon/releases/download/v0.11.3/salmon-0.11.3-linux_x86_64.tar.gz\n",
      "Resolving github.com (github.com)... 192.30.255.112, 192.30.255.113\n",
      "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/32549942/22343d00-abd6-11e8-8376-9b332c400ee0?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20180919%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20180919T032206Z&X-Amz-Expires=300&X-Amz-Signature=0c568d71ebe4011fd14f9151d6cbd72124841229908e4bdefb74d4317b512f76&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dsalmon-0.11.3-linux_x86_64.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2018-09-18 20:22:06--  https://github-production-release-asset-2e65be.s3.amazonaws.com/32549942/22343d00-abd6-11e8-8376-9b332c400ee0?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20180919%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20180919T032206Z&X-Amz-Expires=300&X-Amz-Signature=0c568d71ebe4011fd14f9151d6cbd72124841229908e4bdefb74d4317b512f76&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dsalmon-0.11.3-linux_x86_64.tar.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.229.243\n",
      "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.229.243|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 34421241 (33M) [application/octet-stream]\n",
      "Saving to: ‘salmon-0.11.3-linux_x86_64.tar.gz’\n",
      "\n",
      "salmon-0.11.3-linux 100%[===================>]  32.83M  1.06MB/s    in 44s     \n",
      "\n",
      "2018-09-18 20:22:51 (759 KB/s) - ‘salmon-0.11.3-linux_x86_64.tar.gz’ saved [34421241/34421241]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/COMBINE-lab/salmon/releases/download/v0.11.3/salmon-0.11.3-linux_x86_64.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salmon-0.11.3-linux_x86_64/\n",
      "salmon-0.11.3-linux_x86_64/lib/\n",
      "salmon-0.11.3-linux_x86_64/lib/libtbbmalloc.so\n",
      "salmon-0.11.3-linux_x86_64/lib/libtbb.so\n",
      "salmon-0.11.3-linux_x86_64/lib/libgomp.so.1\n",
      "salmon-0.11.3-linux_x86_64/lib/liblzma.so.0\n",
      "salmon-0.11.3-linux_x86_64/lib/libtbbmalloc_proxy.so.2\n",
      "salmon-0.11.3-linux_x86_64/lib/libtbbmalloc_proxy.so\n",
      "salmon-0.11.3-linux_x86_64/lib/libtbb.so.2\n",
      "salmon-0.11.3-linux_x86_64/lib/libgcc_s.so.1\n",
      "salmon-0.11.3-linux_x86_64/lib/libm.so.6\n",
      "salmon-0.11.3-linux_x86_64/lib/libtbbmalloc.so.2\n",
      "salmon-0.11.3-linux_x86_64/bin/\n",
      "salmon-0.11.3-linux_x86_64/bin/salmon\n",
      "salmon-0.11.3-linux_x86_64/sample_data.tgz\n"
     ]
    }
   ],
   "source": [
    "!tar -xvf salmon-0.11.3-linux_x86_64.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing it out\n",
    "\n",
    "Let's see how salmon works really quick..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salmon v0.11.3\r\n",
      "\r\n",
      "Usage:  salmon -h|--help or \r\n",
      "        salmon -v|--version or \r\n",
      "        salmon -c|--cite or \r\n",
      "        salmon [--no-version-check] <COMMAND> [-h | options]\r\n",
      "\r\n",
      "Commands:\r\n",
      "     index Create a salmon index\r\n",
      "     quant Quantify a sample\r\n",
      "     alevin single cell analysis\r\n",
      "     swim  Perform super-secret operation\r\n",
      "     quantmerge Merge multiple quantifications into a single file\r\n"
     ]
    }
   ],
   "source": [
    "!./salmon-0.11.3-linux_x86_64/bin/salmon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to make an index, but we're really curious about quant, but can't we just try to swim first?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version Info: Could not resolve upgrade information in the alotted time.\r\n",
      "Check for upgrades manually at https://combine-lab.github.io/salmon\r\n",
      "\r\n",
      "    _____       __\r\n",
      "   / ___/____ _/ /___ ___  ____  ____\r\n",
      "   \\__ \\/ __ `/ / __ `__ \\/ __ \\/ __ \\\r\n",
      "  ___/ / /_/ / / / / / / / /_/ / / / /\r\n",
      " /____/\\__,_/_/_/ /_/ /_/\\____/_/ /_/\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!./salmon-0.11.3-linux_x86_64/bin/salmon swim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was hoping for fish. Let's get some test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!wget https://s3.amazonaws.com/nanopore-human-wgs/rna/fastq/UCSC_Run1_20170919_1D.pass.dedup.fastq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version Info: Could not resolve upgrade information in the alotted time.\r\n",
      "Check for upgrades manually at https://combine-lab.github.io/salmon\r\n",
      "    salmon v0.11.3\r\n",
      "    ===============\r\n",
      "\r\n",
      "    salmon quant has two modes --- one quantifies expression using raw reads\r\n",
      "    and the other makes use of already-aligned reads (in BAM/SAM format).\r\n",
      "    Which algorithm is used depends on the arguments passed to salmon quant.\r\n",
      "    If you provide salmon with alignments '-a [ --alignments ]' then the\r\n",
      "    alignment-based algorithm will be used, otherwise the algorithm for\r\n",
      "    quantifying from raw reads will be used.\r\n",
      "\r\n",
      "    to view the help for salmon's quasi-mapping-based mode, use the command\r\n",
      "\r\n",
      "    salmon quant --help-reads\r\n",
      "\r\n",
      "    To view the help for salmon's alignment-based mode, use the command\r\n",
      "\r\n",
      "    salmon quant --help-alignment\r\n",
      "\r\n",
      "    \r\n"
     ]
    }
   ],
   "source": [
    "!./salmon-0.11.3-linux_x86_64/bin/salmon quant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, that's us, raw reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version Info: Could not resolve upgrade information in the alotted time.\r\n",
      "Check for upgrades manually at https://combine-lab.github.io/salmon\r\n",
      "\r\n",
      "Quant\r\n",
      "==========\r\n",
      "Perform dual-phase, mapping-based estimation of\r\n",
      "transcript abundance from RNA-seq reads\r\n",
      "\r\n",
      "salmon quant options:\r\n",
      "\r\n",
      "\r\n",
      "mapping input options:\r\n",
      "  -l [ --libType ] arg                  Format string describing the library \r\n",
      "                                        type\r\n",
      "  -i [ --index ] arg                    salmon index\r\n",
      "  -r [ --unmatedReads ] arg             List of files containing unmated reads \r\n",
      "                                        of (e.g. single-end reads)\r\n",
      "  -1 [ --mates1 ] arg                   File containing the #1 mates\r\n",
      "  -2 [ --mates2 ] arg                   File containing the #2 mates\r\n",
      "\r\n",
      "\r\n",
      "basic options:\r\n",
      "  -v [ --version ]                      print version string\r\n",
      "  -h [ --help ]                         produce help message\r\n",
      "  -o [ --output ] arg                   Output quantification directory.\r\n",
      "  --seqBias                             Perform sequence-specific bias \r\n",
      "                                        correction.\r\n",
      "  --gcBias                              [beta for single-end reads] Perform \r\n",
      "                                        fragment GC bias correction\r\n",
      "  -p [ --threads ] arg (=8)             The number of threads to use \r\n",
      "                                        concurrently.\r\n",
      "  --incompatPrior arg (=0)              This option sets the prior probability \r\n",
      "                                        that an alignment that disagrees with \r\n",
      "                                        the specified library type (--libType) \r\n",
      "                                        results from the true fragment origin. \r\n",
      "                                        Setting this to 0 specifies that \r\n",
      "                                        alignments that disagree with the \r\n",
      "                                        library type should be \"impossible\", \r\n",
      "                                        while setting it to 1 says that \r\n",
      "                                        alignments that disagree with the \r\n",
      "                                        library type are no less likely than \r\n",
      "                                        those that do\r\n",
      "  -g [ --geneMap ] arg                  File containing a mapping of \r\n",
      "                                        transcripts to genes.  If this file is \r\n",
      "                                        provided salmon will output both \r\n",
      "                                        quant.sf and quant.genes.sf files, \r\n",
      "                                        where the latter contains aggregated \r\n",
      "                                        gene-level abundance estimates.  The \r\n",
      "                                        transcript to gene mapping should be \r\n",
      "                                        provided as either a GTF file, or a in \r\n",
      "                                        a simple tab-delimited format where \r\n",
      "                                        each line contains the name of a \r\n",
      "                                        transcript and the gene to which it \r\n",
      "                                        belongs separated by a tab.  The \r\n",
      "                                        extension of the file is used to \r\n",
      "                                        determine how the file should be \r\n",
      "                                        parsed.  Files ending in '.gtf', '.gff'\r\n",
      "                                        or '.gff3' are assumed to be in GTF \r\n",
      "                                        format; files with any other extension \r\n",
      "                                        are assumed to be in the simple format.\r\n",
      "                                        In GTF / GFF format, the \r\n",
      "                                        \"transcript_id\" is assumed to contain \r\n",
      "                                        the transcript identifier and the \r\n",
      "                                        \"gene_id\" is assumed to contain the \r\n",
      "                                        corresponding gene identifier.\r\n",
      "  --meta                                If you're using Salmon on a metagenomic\r\n",
      "                                        dataset, consider setting this flag to \r\n",
      "                                        disable parts of the abundance \r\n",
      "                                        estimation model that make less sense \r\n",
      "                                        for metagenomic data.\r\n",
      "\r\n",
      "\r\n",
      "options specific to mapping mode:\r\n",
      "  --discardOrphansQuasi                 [Quasi-mapping mode only] : Discard \r\n",
      "                                        orphan mappings in quasi-mapping mode. \r\n",
      "                                        If this flag is passed then only paired\r\n",
      "                                        mappings will be considered toward \r\n",
      "                                        quantification estimates.  The default \r\n",
      "                                        behavior is to consider orphan mappings\r\n",
      "                                        if no valid paired mappings exist.  \r\n",
      "                                        This flag is independent of the option \r\n",
      "                                        to write the orphaned mappings to file \r\n",
      "                                        (--writeOrphanLinks).\r\n",
      "  --validateMappings                    [Quasi-mapping mode only] : Validate \r\n",
      "                                        mappings using alignment-based \r\n",
      "                                        verifcation. If this flag is passed, \r\n",
      "                                        quasi-mappings will be validated to \r\n",
      "                                        ensure that they could give rise to a \r\n",
      "                                        reasonable alignment before they are \r\n",
      "                                        further used for quantification.\r\n",
      "  --consensusSlack arg (=0)             [Quasi-mapping mode only] : The amount \r\n",
      "                                        of slack allowed in the quasi-mapping \r\n",
      "                                        consensus mechanism.  Normally, a \r\n",
      "                                        transcript must cover all hits to be \r\n",
      "                                        considered for mapping.  If this is set\r\n",
      "                                        to a value, X, greater than 0, then a \r\n",
      "                                        transcript can fail to cover up to X \r\n",
      "                                        hits before it is discounted as a \r\n",
      "                                        mapping candidate.  The default value \r\n",
      "                                        of this option is 1 if \r\n",
      "                                        --validateMappings is given and 0 \r\n",
      "                                        otherwise.\r\n",
      "  --minScoreFraction arg (=0.65000000000000002)\r\n",
      "                                        [Quasi-mapping mode only] : The \r\n",
      "                                        fraction of the optimal possible \r\n",
      "                                        alignment score that a mapping must \r\n",
      "                                        achieve in order to be considered \r\n",
      "                                        \"valid\" --- should be in (0,1].\r\n",
      "  --ma arg (=\u0002)                         [Quasi-mapping mode only] : The value \r\n",
      "                                        given to a match between read and \r\n",
      "                                        reference nucleotides in an alignment.\r\n",
      "  --mp arg (=�)                         [Quasi-mapping mode only] : The value \r\n",
      "                                        given to a mis-match between read and \r\n",
      "                                        reference nucleotides in an alignment.\r\n",
      "  --go arg (=\u0005)                         [Quasi-mapping mode only] : The value \r\n",
      "                                        given to a gap opening in an alignment.\r\n",
      "  --ge arg (=\u0003)                         [Quasi-mapping mode only] : The value \r\n",
      "                                        given to a gap extension in an \r\n",
      "                                        alignment.\r\n",
      "  --allowOrphansFMD                     [FMD-mapping mode only] : Consider \r\n",
      "                                        orphaned reads as valid hits when \r\n",
      "                                        performing lightweight-alignment.  This\r\n",
      "                                        option will increase sensitivity (allow\r\n",
      "                                        more reads to map and more transcripts \r\n",
      "                                        to be detected), but may decrease \r\n",
      "                                        specificity as orphaned alignments are \r\n",
      "                                        more likely to be spurious.\r\n",
      "  -z [ --writeMappings ] [=arg(=-)]     If this option is provided, then the \r\n",
      "                                        quasi-mapping results will be written \r\n",
      "                                        out in SAM-compatible format.  By \r\n",
      "                                        default, output will be directed to \r\n",
      "                                        stdout, but an alternative file name \r\n",
      "                                        can be provided instead.\r\n",
      "  -c [ --consistentHits ]               Force hits gathered during \r\n",
      "                                        quasi-mapping to be \"consistent\" (i.e. \r\n",
      "                                        co-linear and approximately the right \r\n",
      "                                        distance apart).\r\n",
      "  --strictIntersect                     Modifies how orphans are assigned.  \r\n",
      "                                        When this flag is set, if the \r\n",
      "                                        intersection of the quasi-mappings for \r\n",
      "                                        the left and right is empty, then all \r\n",
      "                                        mappings for the left and all mappings \r\n",
      "                                        for the right read are reported as \r\n",
      "                                        orphaned quasi-mappings\r\n",
      "  --fasterMapping                       [Developer]: Disables some extra checks\r\n",
      "                                        during quasi-mapping. This may make \r\n",
      "                                        mapping a little bit faster at the \r\n",
      "                                        potential cost of returning too many \r\n",
      "                                        mappings (i.e. some sub-optimal \r\n",
      "                                        mappings) for certain reads.\r\n",
      "  -x [ --quasiCoverage ] arg (=0)       [Experimental]: The fraction of the \r\n",
      "                                        read that must be covered by MMPs (of \r\n",
      "                                        length >= 31) if this read is to be \r\n",
      "                                        considered as \"mapped\".  This may help \r\n",
      "                                        to avoid \"spurious\" mappings. A value \r\n",
      "                                        of 0 (the default) denotes no coverage \r\n",
      "                                        threshold (a single 31-mer can yield a \r\n",
      "                                        mapping).  Since coverage by exact \r\n",
      "                                        matching, large, MMPs is a rather \r\n",
      "                                        strict condition, this value should \r\n",
      "                                        likely be set to something low, if \r\n",
      "                                        used.\r\n",
      "\r\n",
      "\r\n",
      "advanced options:\r\n",
      "  --alternativeInitMode                 [Experimental]: Use an alternative \r\n",
      "                                        strategy (rather than simple \r\n",
      "                                        interpolation between) the online and \r\n",
      "                                        uniform abundance estimates to \r\n",
      "                                        initalize the EM / VBEM algorithm.\r\n",
      "  --auxDir arg (=aux_info)              The sub-directory of the quantification\r\n",
      "                                        directory where auxiliary information \r\n",
      "                                        e.g. bootstraps, bias parameters, etc. \r\n",
      "                                        will be written.\r\n",
      "  --dumpEq                              Dump the equivalence class counts that \r\n",
      "                                        were computed during quasi-mapping\r\n",
      "  -d [ --dumpEqWeights ]                Includes \"rich\" equivlance class \r\n",
      "                                        weights in the output when equivalence \r\n",
      "                                        class information is being dumped to \r\n",
      "                                        file.\r\n",
      "  --minAssignedFrags arg (=10)          The minimum number of fragments that \r\n",
      "                                        must be assigned to the transcriptome \r\n",
      "                                        for quantification to proceed.\r\n",
      "  --reduceGCMemory                      If this option is selected, a more \r\n",
      "                                        memory efficient (but slightly slower) \r\n",
      "                                        representation is used to compute \r\n",
      "                                        fragment GC content. Enabling this will\r\n",
      "                                        reduce memory usage, but can also \r\n",
      "                                        reduce speed.  However, the results \r\n",
      "                                        themselves will remain the same.\r\n",
      "  --biasSpeedSamp arg (=5)              The value at which the fragment length \r\n",
      "                                        PMF is down-sampled when evaluating \r\n",
      "                                        sequence-specific & GC fragment bias.  \r\n",
      "                                        Larger values speed up effective length\r\n",
      "                                        correction, but may decrease the \r\n",
      "                                        fidelity of bias modeling results.\r\n",
      "  --fldMax arg (=1000)                  The maximum fragment length to consider\r\n",
      "                                        when building the empirical \r\n",
      "                                        distribution\r\n",
      "  --fldMean arg (=250)                  The mean used in the fragment length \r\n",
      "                                        distribution prior\r\n",
      "  --fldSD arg (=25)                     The standard deviation used in the \r\n",
      "                                        fragment length distribution prior\r\n",
      "  -f [ --forgettingFactor ] arg (=0.65000000000000002)\r\n",
      "                                        The forgetting factor used in the \r\n",
      "                                        online learning schedule.  A smaller \r\n",
      "                                        value results in quicker learning, but \r\n",
      "                                        higher variance and may be unstable.  A\r\n",
      "                                        larger value results in slower learning\r\n",
      "                                        but may be more stable.  Value should \r\n",
      "                                        be in the interval (0.5, 1.0].\r\n",
      "  --initUniform                         initialize the offline inference with \r\n",
      "                                        uniform parameters, rather than seeding\r\n",
      "                                        with online parameters.\r\n",
      "  -w [ --maxReadOcc ] arg (=200)        Reads \"mapping\" to more than this many \r\n",
      "                                        places won't be considered.\r\n",
      "  --noLengthCorrection                  [experimental] : Entirely disables \r\n",
      "                                        length correction when estimating the \r\n",
      "                                        abundance of transcripts.  This option \r\n",
      "                                        can be used with protocols where one \r\n",
      "                                        expects that fragments derive from \r\n",
      "                                        their underlying targets without regard\r\n",
      "                                        to that target's length (e.g. QuantSeq)\r\n",
      "  --noEffectiveLengthCorrection         Disables effective length correction \r\n",
      "                                        when computing the probability that a \r\n",
      "                                        fragment was generated from a \r\n",
      "                                        transcript.  If this flag is passed in,\r\n",
      "                                        the fragment length distribution is not\r\n",
      "                                        taken into account when computing this \r\n",
      "                                        probability.\r\n",
      "  --noFragLengthDist                    [experimental] : Don't consider \r\n",
      "                                        concordance with the learned fragment \r\n",
      "                                        length distribution when trying to \r\n",
      "                                        determine the probability that a \r\n",
      "                                        fragment has originated from a \r\n",
      "                                        specified location.  Normally, \r\n",
      "                                        Fragments with unlikely lengths will be\r\n",
      "                                        assigned a smaller relative probability\r\n",
      "                                        than those with more likely lengths.  \r\n",
      "                                        When this flag is passed in, the \r\n",
      "                                        observed fragment length has no effect \r\n",
      "                                        on that fragment's a priori \r\n",
      "                                        probability.\r\n",
      "  --noBiasLengthThreshold               [experimental] : If this option is \r\n",
      "                                        enabled, then no (lower) threshold will\r\n",
      "                                        be set on how short bias correction can\r\n",
      "                                        make effective lengths. This can \r\n",
      "                                        increase the precision of bias \r\n",
      "                                        correction, but harm robustness.  The \r\n",
      "                                        default correction applies a threshold.\r\n",
      "  --numBiasSamples arg (=2000000)       Number of fragment mappings to use when\r\n",
      "                                        learning the sequence-specific bias \r\n",
      "                                        model.\r\n",
      "  --numAuxModelSamples arg (=5000000)   The first <numAuxModelSamples> are used\r\n",
      "                                        to train the auxiliary model parameters\r\n",
      "                                        (e.g. fragment length distribution, \r\n",
      "                                        bias, etc.).  After ther first \r\n",
      "                                        <numAuxModelSamples> observations the \r\n",
      "                                        auxiliary model parameters will be \r\n",
      "                                        assumed to have converged and will be \r\n",
      "                                        fixed.\r\n",
      "  --numPreAuxModelSamples arg (=1000000)\r\n",
      "                                        The first <numPreAuxModelSamples> will \r\n",
      "                                        have their assignment likelihoods and \r\n",
      "                                        contributions to the transcript \r\n",
      "                                        abundances computed without applying \r\n",
      "                                        any auxiliary models.  The purpose of \r\n",
      "                                        ignoring the auxiliary models for the \r\n",
      "                                        first <numPreAuxModelSamples> \r\n",
      "                                        observations is to avoid applying these\r\n",
      "                                        models before thier parameters have \r\n",
      "                                        been learned sufficiently well.\r\n",
      "  --useEM                               Use the traditional EM algorithm for \r\n",
      "                                        optimization in the batch passes.\r\n",
      "  --useVBOpt                            Use the Variational Bayesian EM \r\n",
      "                                        [default]\r\n",
      "  --rangeFactorizationBins arg (=0)     Factorizes the likelihood used in \r\n",
      "                                        quantification by adopting a new notion\r\n",
      "                                        of equivalence classes based on the \r\n",
      "                                        conditional probabilities with which \r\n",
      "                                        fragments are generated from different \r\n",
      "                                        transcripts.  This is a more \r\n",
      "                                        fine-grained factorization than the \r\n",
      "                                        normal rich equivalence classes.  The \r\n",
      "                                        default value (0) corresponds to the \r\n",
      "                                        standard rich equivalence classes, and \r\n",
      "                                        larger values imply a more fine-grained\r\n",
      "                                        factorization.  If range factorization \r\n",
      "                                        is enabled, a common value to select \r\n",
      "                                        for this parameter is 4.\r\n",
      "  --numGibbsSamples arg (=0)            Number of Gibbs sampling rounds to \r\n",
      "                                        perform.\r\n",
      "  --noGammaDraw                         This switch will disable drawing \r\n",
      "                                        transcript fractions from a Gamma \r\n",
      "                                        distribution during Gibbs sampling.  In\r\n",
      "                                        this case the sampler does not account \r\n",
      "                                        for shot-noise, but only assignment \r\n",
      "                                        ambiguity\r\n",
      "  --numBootstraps arg (=0)              Number of bootstrap samples to \r\n",
      "                                        generate. Note: This is mutually \r\n",
      "                                        exclusive with Gibbs sampling.\r\n",
      "  --bootstrapReproject                  This switch will learn the parameter \r\n",
      "                                        distribution from the bootstrapped \r\n",
      "                                        counts for each sample, but will \r\n",
      "                                        reproject those parameters onto the \r\n",
      "                                        original equivalence class counts.\r\n",
      "  --thinningFactor arg (=16)            Number of steps to discard for every \r\n",
      "                                        sample kept from the Gibbs chain. The \r\n",
      "                                        larger this number, the less chance \r\n",
      "                                        that subsequent samples are \r\n",
      "                                        auto-correlated, but the slower \r\n",
      "                                        sampling becomes.\r\n",
      "  -q [ --quiet ]                        Be quiet while doing quantification \r\n",
      "                                        (don't write informative output to the \r\n",
      "                                        console unless something goes wrong).\r\n",
      "  --perTranscriptPrior                  The prior (either the default or the \r\n",
      "                                        argument provided via --vbPrior) will \r\n",
      "                                        be interpreted as a transcript-level \r\n",
      "                                        prior (i.e. each transcript will be \r\n",
      "                                        given a prior read count of this value)\r\n",
      "  --sigDigits arg (=3)                  The number of significant digits to \r\n",
      "                                        write when outputting the \r\n",
      "                                        EffectiveLength and NumReads columns\r\n",
      "  --vbPrior arg (=1.0000000000000001e-05)\r\n",
      "                                        The prior that will be used in the VBEM\r\n",
      "                                        algorithm.  This is interpreted as a \r\n",
      "                                        per-nucleotide prior, unless the \r\n",
      "                                        --perTranscriptPrior flag is also \r\n",
      "                                        given, in which case this is used as a \r\n",
      "                                        transcript-level prior\r\n",
      "  --writeOrphanLinks                    Write the transcripts that are linked \r\n",
      "                                        by orphaned reads.\r\n",
      "  --writeUnmappedNames                  Write the names of un-mapped reads to \r\n",
      "                                        the file unmapped_names.txt in the \r\n",
      "                                        auxiliary directory.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!./salmon-0.11.3-linux_x86_64/bin/salmon quant --help-reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll grab a transcriptome from gencode https://www.gencodegenes.org/releases/current.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-09-18 20:30:40--  ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_28/gencode.v28.transcripts.fa.gz\n",
      "           => ‘gencode.v28.transcripts.fa.gz’\n",
      "Resolving ftp.ebi.ac.uk (ftp.ebi.ac.uk)... 193.62.192.4\n",
      "Connecting to ftp.ebi.ac.uk (ftp.ebi.ac.uk)|193.62.192.4|:21... connected.\n",
      "Logging in as anonymous ... Logged in!\n",
      "==> SYST ... done.    ==> PWD ... done.\n",
      "==> TYPE I ... done.  ==> CWD (1) /pub/databases/gencode/Gencode_human/release_28 ... done.\n",
      "==> SIZE gencode.v28.transcripts.fa.gz ... 64672964\n",
      "==> PASV ... done.    ==> RETR gencode.v28.transcripts.fa.gz ... done.\n",
      "Length: 64672964 (62M) (unauthoritative)\n",
      "\n",
      "gencode.v28.transcr 100%[===================>]  61.68M   425KB/s    in 2m 48s  \n",
      "\n",
      "2018-09-18 20:33:30 (375 KB/s) - ‘gencode.v28.transcripts.fa.gz’ saved [64672964]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_28/gencode.v28.transcripts.fa.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll make this index, that creates windowed k-mer view of the reference transcriptome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version Info: Could not resolve upgrade information in the alotted time.\n",
      "Check for upgrades manually at https://combine-lab.github.io/salmon\n",
      "index [\"gencode.v28.transcripts.fa.gz.index\"] did not previously exist  . . . creating it\n",
      "[2018-09-18 20:36:44.040] [jLog] [info] building index\n",
      "\u001b[00m[2018-09-18 20:36:44.040] [jointLog] [info] [Step 1 of 4] : counting k-mers\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:45.421] [jointLog] [warning] Entry with header [ENST00000473810.1|ENSG00000239255.1|OTTHUMG00000157482.1|OTTHUMT00000348942.1|RP11-145M9.2-001|RP11-145M9.2|25|processed_pseudogene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:45.512] [jointLog] [warning] Entry with header [ENST00000603775.1|ENSG00000271544.1|OTTHUMG00000184300.1|OTTHUMT00000468575.1|AC006499.9-001|AC006499.9|23|processed_pseudogene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:46.546] [jointLog] [warning] Entry with header [ENST00000632684.1|ENSG00000282431.1|OTTHUMG00000190602.2|OTTHUMT00000485301.2|RP11-520H11.10-001|TRBD1|12|TR_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:47.875] [jointLog] [warning] Entry with header [ENST00000626826.1|ENSG00000281344.1|OTTHUMG00000189570.1|OTTHUMT00000479989.1|RP11-210L7.2-001|HELLPAR|205012|macro_lncRNA|] was longer than 200000 nucleotides.  Are you certain that we are indexing a transcriptome and not a genome?\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:47.934] [jointLog] [warning] Entry with header [ENST00000543745.1|ENSG00000255972.1|OTTHUMG00000168883.1|OTTHUMT00000401485.1|RP11-324E6.8-001|RP11-324E6.8|28|processed_pseudogene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.088] [jointLog] [warning] Entry with header [ENST00000415118.1|ENSG00000223997.1|OTTHUMG00000170844.2|OTTHUMT00000410670.2|AE000661.52-001|TRDD1|8|TR_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.088] [jointLog] [warning] Entry with header [ENST00000434970.2|ENSG00000237235.2|OTTHUMG00000170845.2|OTTHUMT00000410671.2|AE000661.53-001|TRDD2|9|TR_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.088] [jointLog] [warning] Entry with header [ENST00000448914.1|ENSG00000228985.1|OTTHUMG00000170846.2|OTTHUMT00000410672.2|AE000661.54-001|TRDD3|13|TR_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000439842.1|ENSG00000236597.1|OTTHUMG00000152435.2|OTTHUMT00000326213.2|AL122127.38-001|IGHD7-27|11|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000390567.1|ENSG00000211907.1|OTTHUMG00000152429.2|OTTHUMT00000326207.2|AL122127.37-001|IGHD1-26|20|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000452198.1|ENSG00000225825.1|OTTHUMG00000152436.2|OTTHUMT00000326214.2|AL122127.36-001|IGHD6-25|18|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000390569.1|ENSG00000211909.1|OTTHUMG00000152427.2|OTTHUMT00000326205.2|AL122127.35-001|IGHD5-24|20|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000437320.1|ENSG00000227196.1|OTTHUMG00000152438.2|OTTHUMT00000326216.2|AL122127.34-001|IGHD4-23|19|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000390572.1|ENSG00000211912.1|OTTHUMG00000152428.2|OTTHUMT00000326206.2|AL122127.32-001|IGHD2-21|28|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000450276.1|ENSG00000237020.1|OTTHUMG00000152432.2|OTTHUMT00000326210.2|AL122127.31-001|IGHD1-20|17|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000390574.1|ENSG00000211914.1|OTTHUMG00000152431.2|OTTHUMT00000326209.2|AL122127.30-001|IGHD6-19|21|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000390575.1|ENSG00000211915.1|OTTHUMG00000152433.2|OTTHUMT00000326211.2|AL122127.29-001|IGHD5-18|20|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000431870.1|ENSG00000227800.1|OTTHUMG00000152437.2|OTTHUMT00000326215.2|AL122127.28-001|IGHD4-17|16|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000451044.1|ENSG00000227108.1|OTTHUMG00000152369.2|OTTHUMT00000326003.2|AB019441.47-001|IGHD1-14|17|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000390580.1|ENSG00000211920.1|OTTHUMG00000152370.2|OTTHUMT00000326004.2|AB019441.46-001|IGHD6-13|21|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000390581.1|ENSG00000211921.1|OTTHUMG00000152367.2|OTTHUMT00000326001.2|AB019441.45-001|IGHD5-12|23|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000431440.2|ENSG00000232543.2|OTTHUMG00000152368.2|OTTHUMT00000326002.2|AB019441.44-001|IGHD4-11|16|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000430425.1|ENSG00000237197.1|OTTHUMG00000152357.2|OTTHUMT00000325963.2|AB019441.40-001|IGHD1-7|17|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000454691.1|ENSG00000228131.1|OTTHUMG00000152353.2|OTTHUMT00000325959.2|AB019441.39-001|IGHD6-6|18|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000390588.1|ENSG00000211928.1|OTTHUMG00000152360.2|OTTHUMT00000325966.2|AB019441.38-001|IGHD5-5|20|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000414852.1|ENSG00000233655.1|OTTHUMG00000152355.2|OTTHUMT00000325961.2|AB019441.37-001|IGHD4-4|16|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000454908.1|ENSG00000236170.1|OTTHUMG00000152359.2|OTTHUMT00000325965.2|AB019441.34-001|IGHD1-1|17|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.356] [jointLog] [warning] Entry with header [ENST00000518246.1|ENSG00000254045.1|OTTHUMG00000152060.1|OTTHUMT00000325154.1|AB019439.71-001|IGHVIII-22-2|28|IG_V_pseudogene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.358] [jointLog] [warning] Entry with header [ENST00000604642.1|ENSG00000270961.1|OTTHUMG00000184622.2|OTTHUMT00000468982.2|RP11-1360M22.8-001|IGHD5OR15-5A|23|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.358] [jointLog] [warning] Entry with header [ENST00000603326.1|ENSG00000271317.1|OTTHUMG00000184621.3|OTTHUMT00000468981.3|RP11-1360M22.7-001|IGHD4OR15-4A|19|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.358] [jointLog] [warning] Entry with header [ENST00000605284.1|ENSG00000271336.1|OTTHUMG00000184580.2|OTTHUMT00000468908.2|RP11-1360M22.3-001|IGHD1OR15-1A|17|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.359] [jointLog] [warning] Entry with header [ENST00000604446.1|ENSG00000270824.1|OTTHUMG00000184624.2|OTTHUMT00000468984.2|RP11-810K23.15-001|IGHD5OR15-5B|23|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.359] [jointLog] [warning] Entry with header [ENST00000603693.1|ENSG00000270451.1|OTTHUMG00000184611.3|OTTHUMT00000468945.3|RP11-810K23.14-001|IGHD4OR15-4B|19|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.359] [jointLog] [warning] Entry with header [ENST00000604838.1|ENSG00000270185.1|OTTHUMG00000184585.2|OTTHUMT00000468915.2|RP11-1360M22.4-001|IGHD1OR15-1B|17|IG_D_gene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:48.965] [jointLog] [warning] Entry with header [ENST00000579054.1|ENSG00000266416.1|OTTHUMG00000179204.1|OTTHUMT00000445280.1|RP1-66C13.2-001|RP1-66C13.2|28|processed_pseudogene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:49.475] [jointLog] [warning] Entry with header [ENST00000634174.1|ENSG00000282732.1|OTTHUMG00000191398.1|OTTHUMT00000487783.1|RP11-157B13.10-001|RP11-157B13.10|28|unprocessed_pseudogene|], had length less than the k-mer length of 31 (perhaps after poly-A clipping)\n",
      "\u001b[00mElapsed time: 6.2232s\n",
      "\n",
      "\u001b[33m\u001b[1m[2018-09-18 20:36:50.263] [jointLog] [warning] Removed 808 transcripts that were sequence duplicates of indexed transcripts.\n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:36:50.263] [jointLog] [warning] If you wish to retain duplicate transcripts, please use the `--keepDuplicates` flag\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:36:50.264] [jointLog] [info] Replaced 4 non-ATCG nucleotides\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:36:50.264] [jointLog] [info] Clipped poly-A tails from 1,586 transcripts\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:36:50.275] [jointLog] [info] Building rank-select dictionary and saving to disk\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:36:50.291] [jointLog] [info] done\n",
      "\u001b[00mElapsed time: 0.0159987s\n",
      "\u001b[00m[2018-09-18 20:36:50.291] [jointLog] [info] Writing sequence data to file . . . \n",
      "\u001b[00m\u001b[00m[2018-09-18 20:36:50.470] [jointLog] [info] done\n",
      "\u001b[00mElapsed time: 0.178981s\n",
      "\u001b[00m[2018-09-18 20:36:50.487] [jointLog] [info] Building 32-bit suffix array (length of generalized text is 308,972,089)\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:36:50.778] [jointLog] [info] Building suffix array . . . \n",
      "\u001b[00msuccess\n",
      "saving to disk . . . done\n",
      "Elapsed time: 0.664563s\n",
      "done\n",
      "Elapsed time: 29.9049s\n",
      "processed 308,000,000 positions\u001b[00m[2018-09-18 20:39:33.946] [jointLog] [info] khash had 130,317,526 keys\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:39:33.946] [jointLog] [info] saving hash to disk . . . \n",
      "\u001b[00m\u001b[00m[2018-09-18 20:39:41.243] [jointLog] [info] done\n",
      "\u001b[00mElapsed time: 7.29639s\n",
      "[2018-09-18 20:39:43.347] [jLog] [info] done building index\n"
     ]
    }
   ],
   "source": [
    "!./salmon-0.11.3-linux_x86_64/bin/salmon index -t gencode.v28.transcripts.fa.gz -i gencode.v28.transcripts.fa.gz.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So that's a thing that takes a \"long time\" ~1minute but you only have to build once per transcriptome. We probably could have sped things up if we have picked just the coding transcripts.\n",
    "\n",
    "Now we can actually quantify things. We ought to have done this for a mouse transcriptome but at least we're using nanopore RNA?\n",
    "\n",
    "We probably need to think about the strandedness of the fastq data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version Info: Could not resolve upgrade information in the alotted time.\n",
      "Check for upgrades manually at https://combine-lab.github.io/salmon\n",
      "### salmon (mapping-based) v0.11.3\n",
      "### [ program ] => salmon \n",
      "### [ command ] => quant \n",
      "### [ libType ] => { SF }\n",
      "### [ unmatedReads ] => { UCSC_Run1_20170919_1D.pass.dedup.fastq }\n",
      "### [ output ] => { UCSC_Run1_20170919_1D.pass.dedup.fastq.quant }\n",
      "### [ index ] => { gencode.v28.transcripts.fa.gz.index }\n",
      "Logs will be written to UCSC_Run1_20170919_1D.pass.dedup.fastq.quant/logs\n",
      "\u001b[00m[2018-09-18 20:47:39.915] [jointLog] [info] Fragment incompatibility prior below threshold.  Incompatible fragments will be ignored.\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:47:39.915] [jointLog] [info] parsing read library format\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:47:39.915] [jointLog] [info] There is 1 library.\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:47:39.967] [stderrLog] [info] Loading Suffix Array \n",
      "\u001b[00m\u001b[00m[2018-09-18 20:47:39.966] [jointLog] [info] Loading Quasi index\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:47:39.966] [jointLog] [info] Loading 32-bit quasi index\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:47:40.484] [stderrLog] [info] Loading Transcript Info \n",
      "\u001b[00m\u001b[00m[2018-09-18 20:47:40.641] [stderrLog] [info] Loading Rank-Select Bit Array\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:47:40.699] [stderrLog] [info] There were 203,027 set bits in the bit array\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:47:40.731] [stderrLog] [info] Computing transcript lengths\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:47:40.732] [stderrLog] [info] Waiting to finish loading hash\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:47:44.386] [stderrLog] [info] Done loading index\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[00m[2018-09-18 20:47:44.386] [jointLog] [info] done\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:47:44.386] [jointLog] [info] Index contained 203,027 targets\n",
      "\u001b[32mprocessed\u001b[31m 500,000 \u001b[32mfragments\u001b[0m\n",
      "hits: 1,215,614; hits per frag:  2.55548\u001b[00m[2018-09-18 20:48:37.669] [jointLog] [info] Thread saw mini-batch with a maximum of 37.38% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:48:39.281] [jointLog] [info] Thread saw mini-batch with a maximum of 37.60% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:48:39.318] [jointLog] [info] Thread saw mini-batch with a maximum of 37.28% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:48:39.338] [jointLog] [info] Thread saw mini-batch with a maximum of 37.36% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:48:39.499] [jointLog] [info] Thread saw mini-batch with a maximum of 36.68% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:48:39.517] [jointLog] [info] Thread saw mini-batch with a maximum of 37.34% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:48:39.720] [jointLog] [info] Thread saw mini-batch with a maximum of 38.24% zero probability fragments\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:48:39.768] [jointLog] [info] Thread saw mini-batch with a maximum of 37.46% zero probability fragments\n",
      "\u001b[00m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[00m[2018-09-18 20:48:39.819] [jointLog] [info] Computed 42,545 rich equivalence classes for further processing\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:48:39.819] [jointLog] [info] Counted 203,903 total reads in the equivalence classes \n",
      "\u001b[00m\u001b[33m\u001b[1m[2018-09-18 20:48:39.833] [jointLog] [warning] Only 203903 fragments were mapped, but the number of burn-in fragments was set to 5000000.\n",
      "The effective lengths have been computed using the observed mappings.\n",
      "\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:48:39.833] [jointLog] [info] Mapping rate = 28.5739%\n",
      "\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:48:39.833] [jointLog] [info] finished quantifyLibrary()\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:48:39.833] [jointLog] [info] Starting optimizer\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:48:39.878] [jointLog] [info] Marked 0 weighted equivalence classes as degenerate\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:48:39.884] [jointLog] [info] iteration = 0 | max rel diff. = 122.89\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:48:40.317] [jointLog] [info] iteration = 100 | max rel diff. = 1.93525\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:48:40.765] [jointLog] [info] iteration = 200 | max rel diff. = 0.0932501\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:48:41.041] [jointLog] [info] iteration = 266 | max rel diff. = 0.00197179\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:48:41.044] [jointLog] [info] Finished optimizer\n",
      "\u001b[00m\u001b[00m[2018-09-18 20:48:41.044] [jointLog] [info] writing output \n",
      "\n",
      "\u001b[00m"
     ]
    }
   ],
   "source": [
    "!./salmon-0.11.3-linux_x86_64/bin/salmon quant -l SF -r UCSC_Run1_20170919_1D.pass.dedup.fastq \\\n",
    "    -o UCSC_Run1_20170919_1D.pass.dedup.fastq.quant \\\n",
    "    -i gencode.v28.transcripts.fa.gz.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That also is really fast. Less than a minute on my laptop. It also looks like a lot of unmapped reads?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name\tLength\tEffectiveLength\tTPM\tNumReads\r\n",
      "ENST00000456328.2|ENSG00000223972.5|OTTHUMG00000000961.2|OTTHUMT00000362751.1|RP11-34P13.1-002|DDX11L1|1657|processed_transcript|\t1657\t1408.000\t0.000000\t0.000\r\n",
      "ENST00000450305.2|ENSG00000223972.5|OTTHUMG00000000961.2|OTTHUMT00000002844.2|RP11-34P13.1-001|DDX11L1|632|transcribed_unprocessed_pseudogene|\t632\t383.000\t0.000000\t0.000\r\n",
      "ENST00000488147.1|ENSG00000227232.5|OTTHUMG00000000958.1|OTTHUMT00000002839.1|RP11-34P13.2-001|WASH7P|1351|unprocessed_pseudogene|\t1351\t1102.000\t0.000000\t0.000\r\n",
      "ENST00000619216.1|ENSG00000278267.1|-|-|MIR6859-1-201|MIR6859-1|68|miRNA|\t68\t3.848\t0.000000\t0.000\r\n",
      "ENST00000473358.1|ENSG00000243485.5|OTTHUMG00000000959.2|OTTHUMT00000002840.1|RP11-34P13.3-001|RP11-34P13.3|712|lincRNA|\t712\t463.000\t0.000000\t0.000\r\n",
      "ENST00000469289.1|ENSG00000243485.5|OTTHUMG00000000959.2|OTTHUMT00000002841.2|RP11-34P13.3-002|RP11-34P13.3|535|lincRNA|\t535\t286.000\t0.000000\t0.000\r\n",
      "ENST00000607096.1|ENSG00000284332.1|-|-|MIR1302-2-201|MIR1302-2|138|miRNA|\t138\t5.663\t0.000000\t0.000\r\n",
      "ENST00000417324.1|ENSG00000237613.2|OTTHUMG00000000960.1|OTTHUMT00000002842.1|RP11-34P13.4-001|FAM138A|1187|lincRNA|\t1187\t938.000\t0.000000\t0.000\r\n",
      "ENST00000461467.1|ENSG00000237613.2|OTTHUMG00000000960.1|OTTHUMT00000002843.1|RP11-34P13.4-002|FAM138A|590|lincRNA|\t590\t341.000\t2.820974\t0.500\r\n"
     ]
    }
   ],
   "source": [
    "!head UCSC_Run1_20170919_1D.pass.dedup.fastq.quant/quant.sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a tab separated value file, so we can grab it using pandas and do a little quick QC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fae07f9b450>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('UCSC_Run1_20170919_1D.pass.dedup.fastq.quant/quant.sf', delimiter='\\t', header=0)\n",
    "df.hist(bins=50, column='TPM', range=(0.0001, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a pretty good sign that the quantifications we got follow a decent distribution. Here, we're looking at the quantification amount for every gene and just throwing them into a histogram. We should expect that most genes aren't expressed, with some genes being highly expressed. A problem here could show as 3000 genes all getting 30 TPM, but not 29 or 31, so that's good.\n",
    "\n",
    "Let's take a look at the most expressed things as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENST00000546654.1\n"
     ]
    }
   ],
   "source": [
    "loudest_indices = df['TPM'].nlargest(10).index\n",
    "# we have to parse a pipe splitted field\n",
    "loudest_xcripts = [df['Name'][x].split('|')[0] for x in loudest_indices]\n",
    "print(loudest_xcripts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some help from ensembl converting these to genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RPL41', 'TMSB4X', 'RPLP1', 'RPS29', 'RPL41', 'RP11-742N3.1', 'AC002310.17', 'RPL39P3', 'RPS19', 'TMSB10']\n"
     ]
    }
   ],
   "source": [
    "from pyensembl import EnsemblRelease\n",
    "\n",
    "e = EnsemblRelease(release='76')\n",
    "\n",
    "loudest_gene_names = [e.gene_name_of_transcript_id(x.split('.')[0]) for x in loudest_xcripts]\n",
    "print(loudest_gene_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! Lots of boring genes!!! Ribosomal RNA and a cytoskeleton protein.\n",
    "\n",
    "Let's try doing some simple differential analysis. By quantifying another run. We'll download, do the quantification, and load the result. We already made the index, which is nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-09-18 21:54:06--  https://s3.amazonaws.com/nanopore-human-wgs/rna/fastq/UCSC_Run2_20170922_1D.pass.dedup.fastq\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.161.37\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.161.37|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1577008653 (1.5G) [binary/octet-stream]\n",
      "Saving to: ‘UCSC_Run2_20170922_1D.pass.dedup.fastq’\n",
      "\n",
      "                UCS   7%[>                   ] 108.86M   611KB/s    eta 53m 9s "
     ]
    }
   ],
   "source": [
    "%time !wget https://s3.amazonaws.com/nanopore-human-wgs/rna/fastq/UCSC_Run2_20170922_1D.pass.dedup.fastq\n",
    "%time !./salmon-0.11.3-linux_x86_64/bin/salmon quant -l SF -r UCSC_Run2_20170922_1D.pass.dedup.fastq \\\n",
    "    -o UCSC_Run2_20170922_1D.pass.dedup.fastq.quant \\\n",
    "    -i gencode.v28.transcripts.fa.gz.index\n",
    "%time df2 = pd.read_csv('UCSC_Run2_20170922_1D.pass.dedup.fastq.quant/quant.sf', delimiter='\\t', header=0)\n",
    "%time loudest_indices2 = df2['TPM'].nlargest(10).index\n",
    "%time loudest_xcripts2 = [df['Name'][x].split('|')[0] for x in loudest_indices]\n",
    "%time loudest_gene_names2 = [e.gene_name_of_transcript_id(x.split('.')[0]) for x in loudest_xcripts]\n",
    "print(loudest_gene_names2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88430842 0.67216518 0.92837791]\n",
      "[3.195872813441972, 2.455719077557213, 3.7154725627826517, 2.6691288682886327, 2.9793831559625517]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sklearn.metrics.pairwise\n",
    "import scipy.spatial.distance\n",
    "\n",
    "# this is a pretend transcript-sample matrix\n",
    "r = np.random.rand(5,3)\n",
    "\n",
    "print(r[0])\n",
    "\n",
    "# now calculate distance somehow...\n",
    "distances = sklearn.metrics.pairwise.euclidean_distances(r)\n",
    "\n",
    "print([sum(x) for x in distances])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = option1(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(r, 50, density=True, facecolor='g', alpha=0.75)\n",
    "\n",
    "\n",
    "plt.xlabel('Smarts')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Histogram of IQ')\n",
    "plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "plt.axis([40, 160, 0, 0.03])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
